{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS 862 Machine Learning for Business Analysts Fall 2020\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    "#### Submitted by:\n",
    "* Di Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now repeat the same procedure, but this time we will use Yelp review instead. In the same [dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences), there is a file that contains Yelp reviews and the labeled sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN\n",
       "3                                 Crust is not good.        0.0\n",
       "4          Not tasty and the texture was just nasty.        0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "yelp_data = pd.read_csv('yelp_labelled.txt', sep = \"\\t\", names = ['text','sentiment'])\n",
    "\n",
    "yelp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your first task: drop the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0                            Wow... Loved this place.        1.0\n",
       "3                                  Crust is not good.        0.0\n",
       "4           Not tasty and the texture was just nasty.        0.0\n",
       "10  Stopped by during the late May bank holiday of...        1.0\n",
       "11  The selection on the menu was great and so wer...        1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop missing values\n",
    "yelp_data.dropna(inplace = True)\n",
    "yelp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture demonstration, we have seen how to use CountVectorizer and TFIDF, paired with a classifier to perform sentiment analysis. Here we will do the same, but instead of using Random Forest, I want you to use Naive Bayes. Naive Bayes has been a popular algorithm for text classification, since it could achieve good scores and it's a simple model.\n",
    "\n",
    "First separate the data into train and test set. For each feature extraction method, perform hyperparameter tuning that gives you the best classification result on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify X and y\n",
    "X = yelp_data['text']\n",
    "y = yelp_data['sentiment']\n",
    "\n",
    "# Split data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Bag-of-Word as feature extraction, then build a classifier using Naive Bayes. Evaluate the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In python, Bag-of-Word is coded in CountVectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('CV',\n",
       "                                        CountVectorizer(stop_words='english',\n",
       "                                                        token_pattern='\\\\b[^\\\\d\\\\W]+\\\\b')),\n",
       "                                       ('MB', MultinomialNB())]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={'CV__lowercase': (True, False),\n",
       "                         'CV__max_df': [5, 10, 100, 200, 500, 1000],\n",
       "                         'CV__min_df': [1, 2, 3],\n",
       "                         'CV__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
       "                         'MB__alpha': [0, 0.2, 0.4, 0.6, 0.8, 1]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline([('CV', CountVectorizer(stop_words = 'english', token_pattern = r'\\b[^\\d\\W]+\\b')),\n",
    "                     ('MB', MultinomialNB())])\n",
    "parameters = {\n",
    "    'CV__ngram_range':[(1, 1), (1, 2), (1, 3), (1, 4)], \n",
    "    'CV__max_df' : [5,10,100,200,500,1000],\n",
    "    'CV__min_df' : [1,2,3], \n",
    "    'CV__lowercase': (True, False), \n",
    "    'MB__alpha':[0,0.2,0.4,0.6,0.8,1]}\n",
    "Model1 = GridSearchCV(pipeline, parameters, cv = 3, n_jobs = -2)\n",
    "Model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CV__lowercase': True,\n",
       " 'CV__max_df': 100,\n",
       " 'CV__min_df': 1,\n",
       " 'CV__ngram_range': (1, 1),\n",
       " 'MB__alpha': 0.6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Model1.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "- When we convert all characters to lowercase before tokenizing, has 100 maximum document frequency, has 1 minimum document frequency, uses only unigrams, and has 0.6 smoothing parameter. Our model will have an accuracy rate of 79%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TF-IDF as feature extraction, then build a classifier using Naive Bayes. Evaluate the performance on the test set.\n",
    "Note that TF-IDF returns continuous values, so you should be using Gaussian Naive Bayes for the model. However, TF-IDF returns a sparse matrix, where GaussianNB only takes in a dense matrix as input. To overcome this, there are two solutions:\n",
    "1. If you are not doing any tuning, you can add a .toarray() transform after fitting the tfidf (see [here](https://codepunk.io/naive-bayesian-classification-of-text-categories-in-scikit-learn/)).\n",
    "2. If you are using a pipeline, you can put in a function transformer in between to do the convertion (see [here](https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required/28384887#28384887)).\n",
    "\n",
    "I will let you decide which one to use. Note this is not a problem for Multinomial NB because (for whatever reason) Multinomial NB can take in a sprase matrix as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('TF', TfidfVectorizer()),\n",
       "                                       ('FT',\n",
       "                                        FunctionTransformer(accept_sparse=True,\n",
       "                                                            func=<function <lambda> at 0x000001FC64FA9438>)),\n",
       "                                       ('GB', GaussianNB())]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={'GB__var_smoothing': array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 1.e-05, 1.e-06, 1.e-07,\n",
       "       1.e-08, 1.e-09]),\n",
       "                         'TF__lowercase': (True, False),\n",
       "                         'TF__max_df': [5, 10, 100, 200, 500, 1000],\n",
       "                         'TF__min_df': [1, 2, 3],\n",
       "                         'TF__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the pipeline\n",
    "pipeline = Pipeline([('TF', TfidfVectorizer()),\n",
    "                     ('FT', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "                     ('GB', GaussianNB())])\n",
    "parameters = {\n",
    "    'TF__ngram_range':[(1, 1), (1, 2), (1, 3), (1, 4)], \n",
    "    'TF__max_df' : [5,10,100,200,500,1000],\n",
    "    'TF__min_df' : [1,2,3], \n",
    "    'TF__lowercase': (True, False),\n",
    "    'GB__var_smoothing': np.logspace(0,-9, 10)}\n",
    "Model2 = GridSearchCV(pipeline, parameters, cv = 3, n_jobs = -2)\n",
    "Model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GB__var_smoothing': 0.1,\n",
       " 'TF__lowercase': True,\n",
       " 'TF__max_df': 100,\n",
       " 'TF__min_df': 2,\n",
       " 'TF__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Model2.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obbservation:**\n",
    "\n",
    "- When we convert all characters to lowercase before tokenizing, has 100 maximum document frequency, has 2 minimum document frequency, uses unigrams and bigrams, and has 0.1 variances smoothing parameter. Our model will have an accuracy rate of 75.5%.\n",
    "\n",
    "- For this data set, looks like Bag-of-Word perform better than TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
