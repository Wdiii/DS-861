{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS861 Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will be analyzing crime rate data from different communities. We will ran three different Lasso regression models (train/validation/test split, 5-fold cross validation and 10-fold cross validation) and compare their performances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running 3 Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's import some libraries we need to use in this exercise.\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "#basic libraries we use everytime\n",
    "\n",
    "from sklearn import linear_model # For LASSO \n",
    "from sklearn.linear_model import Lasso # For LASSO\n",
    "from sklearn.metrics import mean_squared_error # For evaluation\n",
    "from sklearn.preprocessing import StandardScaler #For standardlization\n",
    "from sklearn.model_selection import train_test_split #For data split\n",
    "from sklearn import metrics # For evaluation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV #For hyperparameters\n",
    "from sklearn.pipeline import Pipeline\n",
    "import itertools \n",
    "\n",
    "import warnings # Suppress warnings because they are annoying\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import time #Help you deal with running time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data set can be found on the UCI machine learning data repository  https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized). \n",
    "\n",
    "The final data set that we will use (community.csv) consists of 2118 observations, and 101 predictors + 1 response (total number of non-violent crimes).\n",
    "\n",
    "Since the data has been clean up, there is no missing value or unnessary response variables, we are ready to use it for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>nonViolPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11980</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>91.78</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.88</td>\n",
       "      <td>12.47</td>\n",
       "      <td>21.44</td>\n",
       "      <td>10.93</td>\n",
       "      <td>11.33</td>\n",
       "      <td>...</td>\n",
       "      <td>10.66</td>\n",
       "      <td>53.72</td>\n",
       "      <td>65.29</td>\n",
       "      <td>78.09</td>\n",
       "      <td>89.14</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1845.9</td>\n",
       "      <td>9.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1394.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23123</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>95.57</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11.01</td>\n",
       "      <td>21.30</td>\n",
       "      <td>10.48</td>\n",
       "      <td>17.18</td>\n",
       "      <td>...</td>\n",
       "      <td>8.30</td>\n",
       "      <td>77.17</td>\n",
       "      <td>71.27</td>\n",
       "      <td>90.22</td>\n",
       "      <td>96.12</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2186.7</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29344</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>94.33</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.35</td>\n",
       "      <td>11.36</td>\n",
       "      <td>25.88</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.28</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>44.77</td>\n",
       "      <td>36.60</td>\n",
       "      <td>61.26</td>\n",
       "      <td>82.85</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2780.9</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6167.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11245</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.53</td>\n",
       "      <td>89.16</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.52</td>\n",
       "      <td>24.46</td>\n",
       "      <td>40.53</td>\n",
       "      <td>28.69</td>\n",
       "      <td>12.65</td>\n",
       "      <td>...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>73.75</td>\n",
       "      <td>42.22</td>\n",
       "      <td>60.34</td>\n",
       "      <td>89.02</td>\n",
       "      <td>11.5</td>\n",
       "      <td>974.2</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9988.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140494</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>95.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.09</td>\n",
       "      <td>32.89</td>\n",
       "      <td>20.04</td>\n",
       "      <td>13.26</td>\n",
       "      <td>...</td>\n",
       "      <td>1.49</td>\n",
       "      <td>64.35</td>\n",
       "      <td>42.29</td>\n",
       "      <td>70.61</td>\n",
       "      <td>85.66</td>\n",
       "      <td>70.4</td>\n",
       "      <td>1995.7</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0       11980           3.10          1.37         91.78          6.50   \n",
       "1       23123           2.82          0.80         95.57          3.44   \n",
       "2       29344           2.43          0.74         94.33          3.43   \n",
       "3       11245           2.76          0.53         89.16          1.17   \n",
       "4      140494           2.45          2.51         95.65          0.90   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  ...  \\\n",
       "0         1.88        12.47        21.44        10.93       11.33  ...   \n",
       "1         0.85        11.01        21.30        10.48       17.18  ...   \n",
       "2         2.35        11.36        25.88        11.01       10.28  ...   \n",
       "3         0.52        24.46        40.53        28.69       12.65  ...   \n",
       "4         0.95        18.09        32.89        20.04       13.26  ...   \n",
       "\n",
       "   PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
       "0           10.66             53.72           65.29          78.09   \n",
       "1            8.30             77.17           71.27          90.22   \n",
       "2            5.00             44.77           36.60          61.26   \n",
       "3            1.74             73.75           42.22          60.34   \n",
       "4            1.49             64.35           42.29          70.61   \n",
       "\n",
       "   PctSameState85  LandArea  PopDens  PctUsePubTrans  LemasPctOfficDrugUn  \\\n",
       "0           89.14       6.5   1845.9            9.63                  0.0   \n",
       "1           96.12      10.6   2186.7            3.84                  0.0   \n",
       "2           82.85      10.6   2780.9            4.37                  0.0   \n",
       "3           89.02      11.5    974.2            0.38                  0.0   \n",
       "4           85.66      70.4   1995.7            0.97                  0.0   \n",
       "\n",
       "   nonViolPerPop  \n",
       "0        1394.59  \n",
       "1        1955.95  \n",
       "2        6167.51  \n",
       "3        9988.79  \n",
       "4        6867.42  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "community = pd.read_csv(\"community.csv\")\n",
    "community.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['population', 'householdsize', 'racepctblack', 'racePctWhite',\n",
       "       'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29',\n",
       "       'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome',\n",
       "       'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec',\n",
       "       'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc',\n",
       "       'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap',\n",
       "       'HispPerCap', 'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade',\n",
       "       'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy',\n",
       "       'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu',\n",
       "       'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr',\n",
       "       'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par',\n",
       "       'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par',\n",
       "       'PctWorkMomYoungKids', 'PctWorkMom', 'NumKidsBornNeverMar',\n",
       "       'PctKidsBornNeverMar', 'NumImmig', 'PctImmigRecent',\n",
       "       'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig',\n",
       "       'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10',\n",
       "       'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam',\n",
       "       'PctLargHouseOccup', 'PersPerOccupHous', 'PersPerOwnOccHous',\n",
       "       'PersPerRentOccHous', 'PctPersOwnOccup', 'PctPersDenseHous',\n",
       "       'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup',\n",
       "       'PctHousOwnOcc', 'PctVacantBoarded', 'PctVacMore6Mos',\n",
       "       'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb',\n",
       "       'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'OwnOccQrange',\n",
       "       'RentLowQ', 'RentMedian', 'RentHighQ', 'RentQrange', 'MedRent',\n",
       "       'MedRentPctHousInc', 'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg',\n",
       "       'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState',\n",
       "       'PctSameHouse85', 'PctSameCity85', 'PctSameState85', 'LandArea',\n",
       "       'PopDens', 'PctUsePubTrans', 'LemasPctOfficDrugUn',\n",
       "       'nonViolPerPop'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to see all the variable names.\n",
    "NameOfVariables = community.columns.values\n",
    "NameOfVariables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outout, we can see all the variables that are been used in this dataset. The detailed explination of them can be seen from the website link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable we use in the study is nonViolPerPop, which shows total number of non-violent crimes per 100K popuation (numeric - decimal) potential GOAL attribut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the X and Y\n",
    "# nonViolPerPop is our response\n",
    "X = community.copy() \n",
    "del X['nonViolPerPop']\n",
    "y = community['nonViolPerPop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have 30% as our hold-out set, and use 70% as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 30% as your hold-out set)\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, \n",
    "                                                                test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method: Train / validation / test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we will choose 60%-20%-20% to split the data. Since we want to have 30% hold-out, we use 56%-14%-30% split for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split the training set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, \n",
    "                                                      test_size = 0.2, random_state = 1) \n",
    "# For the entire dataset, we will do a 56%-14%-30% split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to standardlize the data, we fit the data with training set, then apply this to training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data (only fit with the training data, and transform both the training and validation data)\n",
    "scaler = StandardScaler() # Instantiate\n",
    "scaler.fit(X_train) # Fit the data\n",
    "X_train = pd.DataFrame(scaler.transform(X_train)) # Transform the data\n",
    "X_valid = pd.DataFrame(scaler.transform(X_valid)) # Transform the validation set\n",
    "X_train.columns = X.columns.values\n",
    "X_valid.columns = X.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters for the models includes: alpha (the lambda value in the penalty), max_iter (the maximum number of iterations for optimization algorithm) and tol (the tolerance for optimization).\n",
    "\n",
    "Let's define the hyperparameters we will use for the models. Total, we have 630 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predefine the hyperparameters\n",
    "lambdas = np.logspace(-10,10,21)   # We will use lambda on powers of 10 scale\n",
    "max_iter = [50,70,90]   #We select three numbers\n",
    "tol = np.linspace(0,0.1,10)  #We select 10 numbers from 0 to 0.1\n",
    "hyperparameter_pairs = list(itertools.product(lambdas, max_iter, tol))\n",
    "len(hyperparameter_pairs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum MSE on valid set:  3179290.9583979663\n",
      "Best_pairs: (1.0, 50, 0.0)\n",
      "Total time we use is: 3.8457589149475098\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "Validation_Scores = []     \n",
    "for pairs in hyperparameter_pairs:\n",
    "    lm = Lasso(alpha = pairs[0], max_iter = pairs[1], tol = pairs[2])\n",
    "    lm.fit(X_train, y_train) # Fit model on training set\n",
    "    Validation_Scores.append(metrics.mean_squared_error(lm.predict(X_valid), y_valid)) \n",
    "    # Evaluate model on validation set\n",
    "\n",
    "# Find the minimum validation error, and it's minimizer\n",
    "min_mse = min(Validation_Scores)\n",
    "best_pairs = hyperparameter_pairs[np.argmin(Validation_Scores)]\n",
    "\n",
    "print(\"Minimum MSE on valid set: \", min_mse)    \n",
    "print(\"Best_pairs:\", best_pairs)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time we use is: \"+str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we make a Lasso Regression through Train/validation/test split model, we notice that the total time is around 3.85 seconds, which is quite fast. We want to find the best parameters with the lowest mean squared error. In this case, the alpha is 1, max_iter is 50 and tol is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-744.2533068136923, 'NumKidsBornNeverMar'),\n",
       " (-669.8374700503999, 'PersPerOwnOccHous'),\n",
       " (-464.40249716283387, 'PctImmigRec5'),\n",
       " (-456.8441432163033, 'PctRecentImmig'),\n",
       " (-447.7657445112417, 'PctFam2Par'),\n",
       " (-424.6782049090016, 'PctKids2Par'),\n",
       " (-385.46822694799795, 'PersPerRentOccHous'),\n",
       " (-375.7178109821489, 'PctBSorMore'),\n",
       " (-368.4473411507553, 'PctLess9thGrade'),\n",
       " (-334.88499697124695, 'PctPersOwnOccup'),\n",
       " (-330.61631249825325, 'OwnOccLowQuart'),\n",
       " (-292.08736958795254, 'PopDens'),\n",
       " (-287.0065360886346, 'PctSpeakEnglOnly'),\n",
       " (-262.53180829090746, 'householdsize'),\n",
       " (-256.0140579872967, 'PctLargHouseFam'),\n",
       " (-238.23684429507063, 'PctPersDenseHous'),\n",
       " (-236.78129402293172, 'PctHousLess3BR'),\n",
       " (-234.47649553348887, 'MedOwnCostPctInc'),\n",
       " (-228.75992451973983, 'PctVacMore6Mos'),\n",
       " (-224.15623327125536, 'RentLowQ'),\n",
       " (-198.9808571207515, 'agePct12t29'),\n",
       " (-192.73504529415507, 'medFamInc'),\n",
       " (-191.3077149088405, 'population'),\n",
       " (-188.89679856907168, 'PctEmplManu'),\n",
       " (-182.5233471716151, 'pctWRetire'),\n",
       " (-175.59270668713424, 'PctBornSameState'),\n",
       " (-167.88083749021223, 'PctWorkMom'),\n",
       " (-144.30108542513102, 'PctRecImmig8'),\n",
       " (-134.5339871558987, 'RentHighQ'),\n",
       " (-129.86738196257303, 'pctWPubAsst'),\n",
       " (-114.58015239152894, 'OwnOccMedVal'),\n",
       " (-107.8920261763794, 'PctUsePubTrans'),\n",
       " (-106.12170445227113, 'agePct12t21'),\n",
       " (-103.91457884624386, 'PctHousOccup'),\n",
       " (-94.44497237071862, 'PctNotHSGrad'),\n",
       " (-80.2770184170136, 'AsianPerCap'),\n",
       " (-79.74848022652503, 'TotalPctDiv'),\n",
       " (-75.7630547667567, 'racePctHisp'),\n",
       " (-75.07625611796598, 'medIncome'),\n",
       " (-73.47999523388297, 'MedNumBR'),\n",
       " (-68.77055541341753, 'perCapInc'),\n",
       " (-63.5092851932003, 'MedRentPctHousInc'),\n",
       " (-60.29764928091866, 'pctWWage'),\n",
       " (-54.18210603113587, 'numbUrban'),\n",
       " (-53.60818515841213, 'blackPerCap'),\n",
       " (-49.31272446635463, 'PctSameHouse85'),\n",
       " (-40.49460923806044, 'PctSameState85'),\n",
       " (-39.94746920433062, 'LandArea'),\n",
       " (-30.274268720547767, 'PctWOFullPlumb'),\n",
       " (-15.516737469307346, 'PctRecImmig10'),\n",
       " (-13.941079705268614, 'RentQrange'),\n",
       " (-12.191933861998047, 'NumImmig'),\n",
       " (-10.380112218692993, 'PctVacantBoarded'),\n",
       " (-7.893296015209057, 'agePct16t24'),\n",
       " (-5.6685844097898315, 'PctHousNoPhone'),\n",
       " (-1.017288668592909, 'PctWorkMomYoungKids'),\n",
       " (-0.0, 'PctEmplProfServ'),\n",
       " (4.3265598113767165, 'PctUnemployed'),\n",
       " (15.48777162610534, 'indianPerCap'),\n",
       " (16.36016449378976, 'MedRent'),\n",
       " (17.80697029162933, 'PctImmigRec8'),\n",
       " (20.050535487716314, 'pctWFarmSelf'),\n",
       " (21.99548927869829, 'OwnOccQrange'),\n",
       " (30.59954462905958, 'HispPerCap'),\n",
       " (33.17027418254031, 'OwnOccHiQuart'),\n",
       " (47.43252009056691, 'NumStreet'),\n",
       " (51.7236645385224, 'MedOwnCostPctIncNoMtg'),\n",
       " (59.215183262584475, 'PctYoungKids2Par'),\n",
       " (66.91590994554079, 'PctRecImmig5'),\n",
       " (72.55757501194236, 'pctUrban'),\n",
       " (82.97050688634943, 'PctTeen2Par'),\n",
       " (89.95997532305731, 'racePctAsian'),\n",
       " (110.39790547957695, 'PctSameCity85'),\n",
       " (117.42030077810234, 'agePct65up'),\n",
       " (127.47652076471545, 'pctWInvInc'),\n",
       " (135.23809797039218, 'LemasPctOfficDrugUn'),\n",
       " (162.78078684763807, 'RentMedian'),\n",
       " (190.98521904645602, 'PctNotSpeakEnglWell'),\n",
       " (199.54683920373193, 'PctKidsBornNeverMar'),\n",
       " (200.09883959233895, 'PctOccupManu'),\n",
       " (223.2795455566832, 'PctImmigRecent'),\n",
       " (228.2386887647706, 'PctHousOwnOcc'),\n",
       " (262.0142674678904, 'NumInShelters'),\n",
       " (263.1496869899267, 'PersPerFam'),\n",
       " (271.71095722243774, 'racepctblack'),\n",
       " (277.1267115205932, 'MedYrHousBuilt'),\n",
       " (284.675804091072, 'PctOccupMgmtProf'),\n",
       " (286.4580093183624, 'racePctWhite'),\n",
       " (319.73017003498984, 'FemalePctDiv'),\n",
       " (335.6065495005635, 'HousVacant'),\n",
       " (375.1331731750781, 'PctImmigRec10'),\n",
       " (395.36970520755926, 'NumUnderPov'),\n",
       " (406.2047298226182, 'PctLargHouseOccup'),\n",
       " (524.8443348543163, 'whitePerCap'),\n",
       " (547.224710627162, 'MalePctDivorce'),\n",
       " (565.4736958967651, 'pctWSocSec'),\n",
       " (596.6121219909396, 'PctEmploy'),\n",
       " (666.2302036026053, 'PctPopUnderPov'),\n",
       " (737.8678363817234, 'MalePctNevMarr'),\n",
       " (831.5293093281949, 'PctForeignBorn'),\n",
       " (1057.493663204114, 'PersPerOccupHous')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale data (use the train_valid set to fit, then transform both the train_valid set and test set)\n",
    "scaler = StandardScaler() # Instantiate\n",
    "scaler.fit(X_train_valid) # First fit the data, i.e. learn the mean and sd\n",
    "\n",
    "X_train_valid = pd.DataFrame(scaler.transform(X_train_valid)) # Then transform the data. We can also use fit_transform\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "# Remember, the steps are fit, then transform\n",
    "\n",
    "# Refit Lasso model with selected alpha value (10 from the lambda we get before)\n",
    "lm1 = Lasso(alpha = best_pairs[0], max_iter = best_pairs[1], tol = best_pairs[2])\n",
    "lm1.fit(X_train_valid, y_train_valid)\n",
    "\n",
    "#sort the variables\n",
    "Var_coef = zip(lm1.coef_ , NameOfVariables)\n",
    "sorted(Var_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coefficients shriked to 0 with Train / validation / test split method\n",
    "lm1_coef=pd.DataFrame(zip(lm1.coef_, X_train_valid.columns.values))\n",
    "len(lm1_coef.loc[lm1_coef[0] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error on the testing set is 3539035.5943705076\n"
     ]
    }
   ],
   "source": [
    "print(\"The prediction error on the testing set is\", metrics.mean_squared_error(lm1.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the data with training plus validation set, and transform it to testing set. The result shows a prediction error on the testing set is 3539035.59, which we will compare it with the results of method 2 and 3. The coefficient of PctEmplProfServ is shrink to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second method: 5-Fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_pairs: {'lasso__alpha': 10.0, 'lasso__max_iter': 90, 'lasso__tol': 0.0}\n",
      "Minimum MSE on valid set:  2928751.6743318406\n",
      "Total time we use is: 22.807018518447876\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "best_param=[] # Store the best hyperparameter results\n",
    "\n",
    "# Set up the model pipeline\n",
    "estimator = Pipeline(steps = [('scale', StandardScaler()), # We first need to scale the data\n",
    "                     ('lasso', Lasso()) ]) # Then fit the scaled data using Lasso\n",
    "\n",
    "# Set up the parameters for each item in the pipeline\n",
    "# Parameters of pipelines can be set using â€˜__â€™ separating the parameter names:\n",
    "parameters = {'lasso__alpha': lambdas,\n",
    "             'lasso__max_iter': max_iter,\n",
    "             'lasso__tol': tol} \n",
    "\n",
    "reg = GridSearchCV(estimator = estimator, param_grid = parameters, cv = 5, \n",
    "                   scoring = 'neg_mean_squared_error', n_jobs = -1) # Instantiate the gridsearch\n",
    "reg.fit(X_train_valid, y_train_valid) # Fit the grid search, i.e. perform CV and grid search. \n",
    "print(\"Best_pairs:\", reg.best_params_ ) # The best parameter from CV\n",
    "best_param.append(reg.best_params_)\n",
    "\n",
    "print(\"Minimum MSE on valid set: \",metrics.mean_squared_error(reg.predict(X_valid), y_valid))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time we use is: \"+str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we make a Lasso Regression through 5-fold cross validation model, we notice the total time has increase to around 22.81 seconds bacuse there are more calculations for this method. We want to find the best parameters with the lowest mean squared error. In this case, the alpha is 10, max_iter is 90 and tol is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-702.6363516320591, 'PctKids2Par'),\n",
       " (-363.2007820229292, 'OwnOccLowQuart'),\n",
       " (-284.29858486933784, 'PctLess9thGrade'),\n",
       " (-267.60450338766924, 'NumKidsBornNeverMar'),\n",
       " (-265.9918418234022, 'PopDens'),\n",
       " (-252.21905039413576, 'householdsize'),\n",
       " (-249.42394790166202, 'PersPerOwnOccHous'),\n",
       " (-243.35926773710744, 'PctImmigRec5'),\n",
       " (-240.6962888817962, 'PctRecentImmig'),\n",
       " (-236.92472785157082, 'PctVacMore6Mos'),\n",
       " (-229.2459934431104, 'PctRecImmig8'),\n",
       " (-221.55142080139453, 'RentLowQ'),\n",
       " (-187.61159009394342, 'MedOwnCostPctInc'),\n",
       " (-184.75920841395822, 'agePct12t29'),\n",
       " (-175.98382054702083, 'pctWRetire'),\n",
       " (-162.09283179055518, 'PctNotHSGrad'),\n",
       " (-137.0786492524251, 'PctEmplManu'),\n",
       " (-134.60368935328398, 'PctFam2Par'),\n",
       " (-125.37000068936158, 'RentHighQ'),\n",
       " (-124.08240895953317, 'PctBornSameState'),\n",
       " (-111.31137538923247, 'PctWorkMom'),\n",
       " (-109.94660064216144, 'PctBSorMore'),\n",
       " (-108.45364581214751, 'agePct16t24'),\n",
       " (-100.8192167226675, 'PctUsePubTrans'),\n",
       " (-100.45441863130395, 'PctHousOccup'),\n",
       " (-90.15948877140205, 'PctSpeakEnglOnly'),\n",
       " (-75.33201064311552, 'PctPersOwnOccup'),\n",
       " (-63.741378725580724, 'AsianPerCap'),\n",
       " (-52.458293171469805, 'PctEmplProfServ'),\n",
       " (-51.913724972028064, 'MedNumBR'),\n",
       " (-51.53807589295231, 'PctSameState85'),\n",
       " (-48.08871381799761, 'blackPerCap'),\n",
       " (-31.434445325419297, 'PctPersDenseHous'),\n",
       " (-25.937914323451484, 'OwnOccMedVal'),\n",
       " (-21.34675232766015, 'LandArea'),\n",
       " (-14.507891706339336, 'RentQrange'),\n",
       " (-11.590499433170262, 'PersPerRentOccHous'),\n",
       " (-7.691490232280092, 'PctUnemployed'),\n",
       " (-3.916952543975684, 'PctWOFullPlumb'),\n",
       " (-1.823862364445352, 'pctWPubAsst'),\n",
       " (-0.0, 'MedRent'),\n",
       " (-0.0, 'MedRentPctHousInc'),\n",
       " (-0.0, 'NumImmig'),\n",
       " (0.0, 'NumStreet'),\n",
       " (0.0, 'NumUnderPov'),\n",
       " (-0.0, 'OwnOccHiQuart'),\n",
       " (0.0, 'OwnOccQrange'),\n",
       " (-0.0, 'PctHousLess3BR'),\n",
       " (-0.0, 'PctHousOwnOcc'),\n",
       " (0.0, 'PctImmigRec8'),\n",
       " (0.0, 'PctLargHouseFam'),\n",
       " (0.0, 'PctNotSpeakEnglWell'),\n",
       " (0.0, 'PctOccupManu'),\n",
       " (-0.0, 'PctRecImmig10'),\n",
       " (-0.0, 'PctRecImmig5'),\n",
       " (0.0, 'PctSameHouse85'),\n",
       " (-0.0, 'PctTeen2Par'),\n",
       " (0.0, 'PctVacantBoarded'),\n",
       " (-0.0, 'PctWorkMomYoungKids'),\n",
       " (-0.0, 'PctYoungKids2Par'),\n",
       " (0.0, 'PersPerFam'),\n",
       " (-0.0, 'RentMedian'),\n",
       " (-0.0, 'agePct12t21'),\n",
       " (0.0, 'agePct65up'),\n",
       " (-0.0, 'medFamInc'),\n",
       " (-0.0, 'medIncome'),\n",
       " (-0.0, 'numbUrban'),\n",
       " (-0.0, 'pctWWage'),\n",
       " (-0.0, 'perCapInc'),\n",
       " (-0.0, 'population'),\n",
       " (0.0, 'racePctAsian'),\n",
       " (0.0, 'racePctHisp'),\n",
       " (0.0, 'racePctWhite'),\n",
       " (4.363030714360807, 'pctWFarmSelf'),\n",
       " (11.062596974190072, 'indianPerCap'),\n",
       " (13.362856275833268, 'TotalPctDiv'),\n",
       " (23.958213198046458, 'HispPerCap'),\n",
       " (32.993598392762976, 'MedOwnCostPctIncNoMtg'),\n",
       " (46.36964907467078, 'pctWInvInc'),\n",
       " (60.468197561230795, 'PctHousNoPhone'),\n",
       " (64.42991726380978, 'PctLargHouseOccup'),\n",
       " (70.3112459086443, 'PctKidsBornNeverMar'),\n",
       " (74.64216014031835, 'NumInShelters'),\n",
       " (77.33201519506493, 'PctImmigRecent'),\n",
       " (82.94280720238703, 'PctOccupMgmtProf'),\n",
       " (85.16390906452072, 'pctUrban'),\n",
       " (90.9955376781834, 'racepctblack'),\n",
       " (105.12786871228158, 'PctSameCity85'),\n",
       " (136.16067407089068, 'FemalePctDiv'),\n",
       " (155.310170272931, 'LemasPctOfficDrugUn'),\n",
       " (161.78330159444334, 'MedYrHousBuilt'),\n",
       " (223.46444177595768, 'HousVacant'),\n",
       " (266.5138563673401, 'PctImmigRec10'),\n",
       " (353.6770569635012, 'whitePerCap'),\n",
       " (404.77478012578564, 'PctEmploy'),\n",
       " (426.0659057137524, 'PctPopUnderPov'),\n",
       " (439.2131086385415, 'MalePctDivorce'),\n",
       " (524.4200587033115, 'pctWSocSec'),\n",
       " (544.2065099627077, 'PersPerOccupHous'),\n",
       " (553.964898808239, 'MalePctNevMarr'),\n",
       " (809.311619537454, 'PctForeignBorn')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refit the model on the whole training set, which the selected lambda\n",
    "scaler = StandardScaler().fit(X_train_valid)\n",
    "X_trainS = pd.DataFrame(scaler.transform(X_train_valid))\n",
    "X_testS = pd.DataFrame(scaler.transform(X_test))\n",
    "X_trainS.columns = X.columns.values\n",
    "\n",
    "# Model fitting\n",
    "model1 = Lasso(alpha = 10, max_iter = 90, tol = 0)\n",
    "model1.fit(X_trainS, y_train_valid)\n",
    "\n",
    "#sort the variables\n",
    "Var_coef = zip(model1.coef_ , NameOfVariables)\n",
    "sorted(Var_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coefficients shriked to 0 with 5 Fold CV\n",
    "model1_coef=pd.DataFrame(zip(model1.coef_, X_train_valid.columns.values))\n",
    "len(model1_coef.loc[model1_coef[0] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error on the testing set is 3472450.8070895094\n"
     ]
    }
   ],
   "source": [
    "# Prediction evaluation\n",
    "print(\"The prediction error on the testing set is\", metrics.mean_squared_error(model1.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the data with the whole training set, and transform it to testing set. The result shows a prediction error on the testing set is 3472450.81, which is much better than the result we get from method 1. Totally, the coefficients of 33 variables have shrinked to zero. Therefore, among the 101 predictors, we only need to use 68 predictors, which can decrease the complexity of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third method: 10-Fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_pairs: {'lasso__alpha': 10.0, 'lasso__max_iter': 70, 'lasso__tol': 0.044444444444444446}\n",
      "Minimum MSE on valid set:  2923167.118981143\n",
      "Total time we use is: 45.714776039123535\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "best_param=[] # Store the best hyperparameter results\n",
    "\n",
    "# Set up the model pipeline\n",
    "estimator = Pipeline(steps = [('scale', StandardScaler()), # We first need to scale the data\n",
    "                     ('lasso', Lasso()) ]) # Then fit the scaled data using Lasso\n",
    "\n",
    "parameters = {'lasso__alpha': lambdas,\n",
    "             'lasso__max_iter': max_iter,\n",
    "             'lasso__tol': tol}  \n",
    "\n",
    "reg = GridSearchCV(estimator = estimator, param_grid = parameters, cv = 10, \n",
    "                   scoring = 'neg_mean_squared_error', n_jobs = -1) # Instantiate the gridsearch\n",
    "reg.fit(X_train_valid, y_train_valid) # Fit the grid search, i.e. perform CV and grid search. \n",
    "print(\"Best_pairs:\", reg.best_params_ ) # The best parameter from CV\n",
    "best_param.append(reg.best_params_)\n",
    "\n",
    "print(\"Minimum MSE on valid set: \",metrics.mean_squared_error(reg.predict(X_valid), y_valid))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time we use is: \"+str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we make a Lasso Regression through 10-fold cross validation model, we notice the total time has increase to around 45.71 seconds, because 10-fold will double our calaulations. We want to find the best parameters with the lowest mean squared error. In this case, the alpha is 10, max_iter is 70 and tol is 0.044."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-526.1923956565603, 'PctKids2Par'),\n",
       " (-367.37047348658854, 'OwnOccLowQuart'),\n",
       " (-331.40672975616064, 'PctFam2Par'),\n",
       " (-305.7887622645784, 'agePct12t29'),\n",
       " (-304.81411242551195, 'PctLess9thGrade'),\n",
       " (-291.06160126598377, 'PctRecentImmig'),\n",
       " (-277.8154344878706, 'PopDens'),\n",
       " (-263.5235937572656, 'PersPerOwnOccHous'),\n",
       " (-259.05538180827074, 'PctImmigRec5'),\n",
       " (-258.0783556571665, 'householdsize'),\n",
       " (-244.3204854702896, 'NumKidsBornNeverMar'),\n",
       " (-234.41807627569645, 'PctVacMore6Mos'),\n",
       " (-207.57343405943195, 'RentLowQ'),\n",
       " (-187.58826246570646, 'MedOwnCostPctInc'),\n",
       " (-169.4688411683862, 'pctWRetire'),\n",
       " (-135.0886480304948, 'PctEmplManu'),\n",
       " (-132.22704689633932, 'PctBSorMore'),\n",
       " (-126.90119801493773, 'PctSpeakEnglOnly'),\n",
       " (-121.37903151448971, 'PctBornSameState'),\n",
       " (-108.48742295752014, 'PctRecImmig8'),\n",
       " (-103.78414382450681, 'PctWorkMom'),\n",
       " (-101.17607155370594, 'PctHousOccup'),\n",
       " (-96.73180329571643, 'PctUsePubTrans'),\n",
       " (-90.56055737332377, 'PctPersOwnOccup'),\n",
       " (-79.94793276401006, 'pctWPubAsst'),\n",
       " (-66.1997660118439, 'AsianPerCap'),\n",
       " (-64.02026948177986, 'PctSameState85'),\n",
       " (-60.04693896351487, 'PctPersDenseHous'),\n",
       " (-57.172387128442864, 'RentHighQ'),\n",
       " (-49.82235504188656, 'blackPerCap'),\n",
       " (-47.65303834309403, 'MedNumBR'),\n",
       " (-41.006368748384844, 'PersPerRentOccHous'),\n",
       " (-40.74946246678116, 'agePct16t24'),\n",
       " (-40.045944789215675, 'PctNotHSGrad'),\n",
       " (-23.953746209211292, 'PctEmplProfServ'),\n",
       " (-23.746904953923273, 'OwnOccMedVal'),\n",
       " (-20.840181538098417, 'MedRentPctHousInc'),\n",
       " (-18.92553851063478, 'LandArea'),\n",
       " (-16.635314023627252, 'RentQrange'),\n",
       " (-9.337550330085778, 'PctWOFullPlumb'),\n",
       " (-7.972508200679637, 'PctUnemployed'),\n",
       " (-4.835827797220537, 'medFamInc'),\n",
       " (-1.2311396180102945, 'agePct12t21'),\n",
       " (-0.0, 'MedRent'),\n",
       " (-0.0, 'NumImmig'),\n",
       " (0.0, 'NumStreet'),\n",
       " (0.0, 'NumUnderPov'),\n",
       " (-0.0, 'OwnOccHiQuart'),\n",
       " (0.0, 'OwnOccQrange'),\n",
       " (-0.0, 'PctHousLess3BR'),\n",
       " (-0.0, 'PctHousOwnOcc'),\n",
       " (-0.0, 'PctImmigRec8'),\n",
       " (0.0, 'PctLargHouseFam'),\n",
       " (0.0, 'PctNotSpeakEnglWell'),\n",
       " (0.0, 'PctOccupManu'),\n",
       " (-0.0, 'PctRecImmig10'),\n",
       " (-0.0, 'PctRecImmig5'),\n",
       " (0.0, 'PctSameHouse85'),\n",
       " (-0.0, 'PctTeen2Par'),\n",
       " (0.0, 'PctVacantBoarded'),\n",
       " (-0.0, 'PctWorkMomYoungKids'),\n",
       " (-0.0, 'PctYoungKids2Par'),\n",
       " (-0.0, 'PersPerFam'),\n",
       " (-0.0, 'RentMedian'),\n",
       " (0.0, 'agePct65up'),\n",
       " (-0.0, 'medIncome'),\n",
       " (-0.0, 'numbUrban'),\n",
       " (0.0, 'pctWWage'),\n",
       " (-0.0, 'perCapInc'),\n",
       " (-0.0, 'population'),\n",
       " (0.0, 'racePctAsian'),\n",
       " (-0.0, 'racePctHisp'),\n",
       " (0.0, 'racePctWhite'),\n",
       " (1.1321444879128346, 'TotalPctDiv'),\n",
       " (6.141333742626135, 'pctWFarmSelf'),\n",
       " (6.822010807918532, 'indianPerCap'),\n",
       " (20.627403358437974, 'HispPerCap'),\n",
       " (22.261623827003657, 'PctHousNoPhone'),\n",
       " (32.5812620643193, 'MedOwnCostPctIncNoMtg'),\n",
       " (62.11048390382722, 'NumInShelters'),\n",
       " (64.458488471575, 'PctOccupMgmtProf'),\n",
       " (70.77906086672799, 'PctKidsBornNeverMar'),\n",
       " (78.9643266498598, 'PctLargHouseOccup'),\n",
       " (88.17507869123037, 'racepctblack'),\n",
       " (88.46700101835155, 'pctUrban'),\n",
       " (91.96021789419328, 'PctSameCity85'),\n",
       " (92.22417337492426, 'PctImmigRecent'),\n",
       " (104.03536446308676, 'pctWInvInc'),\n",
       " (155.76516657895752, 'LemasPctOfficDrugUn'),\n",
       " (163.9793240567743, 'MedYrHousBuilt'),\n",
       " (183.74146408742737, 'FemalePctDiv'),\n",
       " (211.51427668498533, 'HousVacant'),\n",
       " (262.93305526362286, 'PctImmigRec10'),\n",
       " (335.1324788928199, 'whitePerCap'),\n",
       " (376.72163294060954, 'PctEmploy'),\n",
       " (434.7752258913993, 'MalePctDivorce'),\n",
       " (459.3992440569911, 'pctWSocSec'),\n",
       " (525.471332479712, 'PctPopUnderPov'),\n",
       " (550.7840238335692, 'MalePctNevMarr'),\n",
       " (609.3633966807487, 'PersPerOccupHous'),\n",
       " (705.4946597095271, 'PctForeignBorn')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refit the model on the whole training set, which the selected lambda\n",
    "scaler = StandardScaler().fit(X_train_valid)\n",
    "X_trainS = pd.DataFrame(scaler.transform(X_train_valid))\n",
    "X_testS = pd.DataFrame(scaler.transform(X_test))\n",
    "X_trainS.columns = X.columns.values\n",
    "\n",
    "# Model fitting\n",
    "best_param = best_param[0] \n",
    "model2 = Lasso(alpha = best_param['lasso__alpha'], max_iter = best_param['lasso__max_iter'],\n",
    "               tol = best_param['lasso__tol'])\n",
    "model2.fit(X_train_valid, y_train_valid)\n",
    "\n",
    "#sort the variables\n",
    "Var_coef = zip(model2.coef_ , NameOfVariables)\n",
    "sorted(Var_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coefficients shriked to 0 with 10 Fold CV\n",
    "model2_coef=pd.DataFrame(zip(model2.coef_, X_train_valid.columns.values))\n",
    "len(model2_coef.loc[model2_coef[0] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error on the testing set is 3476141.483637399\n"
     ]
    }
   ],
   "source": [
    "# Prediction evaluation\n",
    "print(\"The prediction error on the testing set is\", metrics.mean_squared_error(model2.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the data with the whole training set, and transform it to testing set. The result shows a prediction error on the testing set is 3476141.48, which is good, but higher than the result of method 2. Totally, the coefficients of 30 variables have shrinked to zero. Therefore, among the 101 predictors, we only need to use 71 predictors, which can decrease the complexity of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyperparameter selection times\n",
    "\n",
    "The model running time for the three different Lasso regression models (train/validation/test split, 5-fold cross validation and 10-fold cross validation) are 3.85, 22.81 and 45.71 seconds. As the number of model calculations increases, the running time will increase as well. Therefor, if the users wants to save time, it's better for them to use train/validation/test split model, not the cross validation model. \n",
    "\n",
    "2. Coeffcients \n",
    "\n",
    "Among these 101 variables, we noticed that the coefficient values for these variables shrinked when the cross validation model is run. It is good for Lasso regression as it is a type of linear regression that uses shrinkage. Lasso regression wants to make data values closed to a central point, like the mean. Also, the lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). \n",
    "\n",
    "Therefore, comparing to train/validation/test split, 5-fold cross validation and 10-fold cross validation do decrease the number of predictors, which is good for the users.\n",
    "\n",
    "3. Prediction error\n",
    "\n",
    "The prediction of mean squared error for the three different Lasso regression models are 3539035.59, 3472450.81 and 3476141.48, which are pretty close. 5-fold cross validation has the best prediction performance among the three models, train/validation/test split has the worst. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, running time is the advantage of the train/validation/test split method and prediction performance is the advantage of the cross validation method. we would suggest users to choose the 5-fold cross validation method from these three methods as it has better performance and takes less time to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
